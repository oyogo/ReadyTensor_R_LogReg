#!/usr/bin/env Rscript

library(jsonlite)
library(dplyr)
library(tidyr)
library(caret)
library(readr)
library(data.table)
library(fastDummies)
library(nnet)

# Define directories and paths

ROOT_DIR <- dirname(getwd())
MODEL_INPUTS_OUTPUTS <- file.path(ROOT_DIR, 'model_inputs_outputs')
INPUT_DIR <- file.path(MODEL_INPUTS_OUTPUTS, "inputs")
INPUT_SCHEMA_DIR <- file.path(INPUT_DIR, "schema")
DATA_DIR <- file.path(INPUT_DIR, "data")
TRAIN_DIR <- file.path(DATA_DIR, "training")
MODEL_ARTIFACTS_PATH <- file.path(MODEL_INPUTS_OUTPUTS, "model", "artifacts")
OHE_ENCODER_FILE <- file.path(MODEL_ARTIFACTS_PATH, 'ohe.rds')
PREDICTOR_FILE_PATH <- file.path(MODEL_ARTIFACTS_PATH, "predictor", "predictor.rds")
IMPUTATION_FILE <- file.path(MODEL_ARTIFACTS_PATH, 'imputation.rds')
LABEL_ENCODER_FILE <- file.path(MODEL_ARTIFACTS_PATH, 'label_encoder.rds')
ENCODED_TARGET_FILE <- file.path(MODEL_ARTIFACTS_PATH, "encoded_target.rds")
TOP_3_CATEGORIES_MAP <- file.path(MODEL_ARTIFACTS_PATH, "top_3_map.rds")


if (!dir.exists(MODEL_ARTIFACTS_PATH)) {
  dir.create(MODEL_ARTIFACTS_PATH, recursive = TRUE)
}
if (!dir.exists(file.path(MODEL_ARTIFACTS_PATH, "predictor"))) {
  dir.create(file.path(MODEL_ARTIFACTS_PATH, "predictor"))
}


# Reading the schema
# The schema contains metadata about the datasets. 
# We will use the scehma to get information about the type of each feature (NUMERIC or CATEGORICAL)
# and the id and target features, this will be helpful in preprocessing stage.

file_name <- list.files(INPUT_SCHEMA_DIR, pattern = "*.json")[1]
#schema <- fromJSON(file.path(INPUT_SCHEMA_DIR, file_name))

# read in the schema so that we extract the response variable
dataschema <- fromJSON(file.path(INPUT_SCHEMA_DIR, file_name))

features <- dataschema$features

numeric_features <- features$name[features$dataType == "NUMERIC"]
categorical_features <- features$name[features$dataType == "CATEGORICAL"]
id_feature <- dataschema$id$name
target_feature <- dataschema$target$name
model_category <- dataschema$modelCategory


# Reading training data
file_name <- list.files(TRAIN_DIR, pattern = "*.csv")[1]
# Read the first line to get column names
header_line <- readLines(file.path(TRAIN_DIR, file_name), n = 1)
col_names <- unlist(strsplit(header_line, split = ",")) # assuming ',' is the delimiter
# Read the CSV with the exact column names
df <- read.csv(file.path(TRAIN_DIR, file_name), skip = 1, col.names = col_names, check.names=FALSE)

# Data Preprocessing
# Data preprocessing is very important before training the model, as the data may contain missing values in some cells. 
# Moreover, most of the learning algorithms cannot work with categorical data, thus the data has to be encoded.
# In this section we will impute the missing values and encode the categorical features. Afterwards the data will be ready to train the model.

# You can add your own preprocessing steps such as:

# Normalization
# Outlier removal
# Handling imbalanced classes
# Dropping or adding features

# Important note:
# Saving the values used for imputation during training step is crucial. 
# These values will be used to impute missing data in the testing set. 
# This is very important to avoid the well known problem of data leakage. 
# During testing, you should not make any assumptions about the data in hand, 
# alternatively anything needed during the testing phase should be learned from the training phase.
# This is why we are creating a dictionary of values used during training to reuse these values during testing.

source('preprocessor.R')

df.tn <- preprocessing(df.tn=df,df.test=0)

# The id column is just an identifier for the training example, so we will exclude it during the encoding phase.
# Target feature will be label encoded in the next step.

encoded_target <- readRDS(ENCODED_TARGET_FILE)

# fit the model 
theModel <- glm(encoded_target ~ ., family = binomial(link = "logit"), data = df.tn)

# save the model into the artifacts folder in the attached volume.
saveRDS(theModel, PREDICTOR_FILE_PATH)


